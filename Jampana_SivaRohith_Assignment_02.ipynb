{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5887a6e4-4214-4b5e-e9e0-8f14217c1a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching abstracts from paper 0.\n",
            "Fetching abstracts from paper 100.\n",
            "Fetching abstracts from paper 200.\n",
            "Fetching abstracts from paper 300.\n",
            "Fetching abstracts from paper 400.\n",
            "Fetching abstracts from paper 500.\n",
            "Fetching abstracts from paper 600.\n",
            "Fetching abstracts from paper 700.\n",
            "Fetching abstracts from paper 800.\n",
            "Fetching abstracts from paper 900.\n",
            "Error fetching data: 400, Offset: 1000\n",
            "Fetched 1000 abstracts. Saved to data_science_abstracts.csv.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Your API key\n",
        "api_key = \"OSi7PPNtPI3r2KsQa1OKU8hWTL5WoSO788SpEnCW\"\n",
        "\n",
        "# API request headers\n",
        "headers = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"x-api-key\": api_key\n",
        "}\n",
        "\n",
        "# Search query\n",
        "query = \"data science\"\n",
        "\n",
        "# API endpoint\n",
        "api_endpoint = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Request parameters\n",
        "params = {\n",
        "    \"query\": query,\n",
        "    \"fields\": \"title,abstract\",\n",
        "    \"limit\": 100  # Max results per request\n",
        "}\n",
        "\n",
        "# List to store all collected papers\n",
        "paper = []\n",
        "\n",
        "# Loop to fetch batches of results\n",
        "for offset in range(0, 10000, 100):\n",
        "    params[\"offset\"] = offset\n",
        "    response = requests.get(api_endpoint, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        papers = response.json().get('data', [])\n",
        "        if papers:\n",
        "            paper.extend(papers)\n",
        "            print(f\"Fetching abstracts from paper {offset}.\")\n",
        "        else:\n",
        "            print(f\"No papers found for offset {offset}.\")\n",
        "            break\n",
        "    elif response.status_code == 429:\n",
        "        print(f\"Rate limit exceeded. Waiting 1 second for offset {offset}.\")\n",
        "        time.sleep(1)  # Waiting time\n",
        "        response = requests.get(api_endpoint, headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            papers = response.json().get('data', [])\n",
        "            if papers:\n",
        "                paper.extend(papers)\n",
        "                print(f\"Collected papers after retry for offset {offset}.\")\n",
        "            else:\n",
        "                print(f\"No papers found after retry for offset {offset}.\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Error after retrying: {response.status_code}, Offset: {offset}\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Error fetching data: {response.status_code}, Offset: {offset}\")\n",
        "        break\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "# Extract titles and abstracts\n",
        "abstracts = [(paper[\"title\"], paper.get(\"abstract\", \"N/A\")) for paper in paper]\n",
        "\n",
        "# Save to CSV file\n",
        "output_file = \"data_science_abstracts.csv\"\n",
        "df = pd.DataFrame(abstracts, columns=[\"Title\", \"Abstract\"])\n",
        "df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Fetched {len(abstracts)} abstracts. Saved to {output_file}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e3280f-63d6-4d6d-8bd8-9939dda2256a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data has been saved to 'cleaned_data_science_abstracts.csv\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "!pip install pandas nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('data_science_abstracts.csv')\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''  # Return an empty string if the input is not a valid string\n",
        "\n",
        "    # 1. Remove special characters and punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 2. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3. Remove stopwords\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # 4. Convert to lowercase\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # 5. Apply stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # 6. Apply lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Convert the 'Abstract' column to strings, filling NaN values with an empty string\n",
        "data['Abstract'] = data['Abstract'].astype(str).fillna('')\n",
        "\n",
        "# Apply the cleaning function to the Abstract column\n",
        "data['Cleaned_Abstract'] = data['Abstract'].apply(clean_text)\n",
        "print(\"Cleaned saved to 'cleaned_data_science_abstracts.csv\")\n",
        "# Save the cleaned data into a new CSV file\n",
        "data.to_csv('cleaned_data_science_abstracts.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90060289-5c3c-421a-80ba-2efa72f0ea03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is saved in analyzed_data_science_abstracts.csv\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK resources (uncomment if you haven't already)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "file = \"cleaned_data_science_abstracts.csv\"\n",
        "data = pd.read_csv(file)\n",
        "\n",
        "# Load the spaCy model for NER and dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging and count POS types\n",
        "def pos_analysis(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)  # POS tagging\n",
        "\n",
        "    # Counting specific POS tags\n",
        "    counts = Counter(tag for word, tag in tagged)\n",
        "    noun_count = sum(counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
        "    verb_count = sum(counts[tag] for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
        "    adjective_count = sum(counts[tag] for tag in ['JJ', 'JJR', 'JJS'])\n",
        "    adverb_count = sum(counts[tag] for tag in ['RB', 'RBR', 'RBS'])\n",
        "\n",
        "    pos_counts = {\n",
        "        'Nouns': noun_count,\n",
        "        'Verbs': verb_count,\n",
        "        'Adjectives': adjective_count,\n",
        "        'Adverbs': adverb_count\n",
        "    }\n",
        "\n",
        "    return tagged, pos_counts\n",
        "\n",
        "# Function to perform dependency parsing using spaCy\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "# Function to perform constituency parsing using nltk\n",
        "def constituency_parsing(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "\n",
        "    def tree_to_string(tree):\n",
        "        \"\"\" Convert an NLTK tree to a string to visualize the tree structure. \"\"\"\n",
        "        if isinstance(tree, Tree):\n",
        "            return f\"({tree.label()} {' '.join(tree_to_string(child) for child in tree)})\"\n",
        "        else:\n",
        "            return tree[0]\n",
        "\n",
        "    return tree_to_string(chunked)\n",
        "\n",
        "# Function to extract named entities and their counts\n",
        "def ner_analysis(text):\n",
        "    doc = nlp(text)\n",
        "    entities = Counter(ent.label_ for ent in doc.ents)\n",
        "    return entities, [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Perform analysis for each abstract and store results\n",
        "results = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    abstract = row['Cleaned_Abstract']\n",
        "\n",
        "    # Skip empty abstracts\n",
        "    if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    # POS Analysis\n",
        "    tagged, pos_counts = pos_analysis(abstract)\n",
        "\n",
        "    # Dependency Parsing\n",
        "    dependency_parse = dependency_parsing(abstract)\n",
        "\n",
        "    # Constituency Parsing\n",
        "    constituency_parse = constituency_parsing(abstract)\n",
        "\n",
        "    # NER Analysis\n",
        "    ner_counts, ner_entities = ner_analysis(abstract)\n",
        "\n",
        "    results.append({\n",
        "        'Abstract': abstract,\n",
        "        'POS Tagged': tagged,\n",
        "        'POS Counts': pos_counts,\n",
        "        'Dependency Parsing': dependency_parse,\n",
        "        'Constituency Parsing': constituency_parse,\n",
        "        'Named Entities': ner_entities,\n",
        "        'NER Counts': ner_counts\n",
        "    })\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"analyzed_data_science_abstracts.csv\", index=False)\n",
        "print(\"Data is saved in analyzed_data_science_abstracts.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points\n",
        "\n",
        "[cleaned_data_science_abstracts.csv](https://myunt-my.sharepoint.com/:x:/g/personal/sivarohithjampana_my_unt_edu/EbN1l-sgbZ1EnynIgvXzX8ABhZmp_LgtvSAGmL91u6pYRA?e=Molmge)\n",
        "\n",
        "\n",
        "[analyzed_data_science_abstracts](https://myunt-my.sharepoint.com/:x:/g/personal/sivarohithjampana_my_unt_edu/EZDO4mbA84REpw_6ZzzidBEBphe9YSgtXE6s3Dvm48BaAA?e=XVfQ4E)"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'''The assignment was a good learning experience for webscraping and data analysis.I got experience of using API.I enjoyed applying NLP techniques like POS tagging and Named Entity Recognition, finding insights from the data. The allocated time was reasonable, allowing thorough exploration while feeling slightly rushed on parsing nuances.'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fc3fb914-0d2b-411a-fb1d-9ce8234d42db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The assignment was a good learning experience for webscraping and data analysis.I got experience of using API.I enjoyed applying NLP techniques like POS tagging and Named Entity Recognition, finding insights from the data. The allocated time was reasonable, allowing thorough exploration while feeling slightly rushed on parsing nuances.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsMrCAkjqW8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}